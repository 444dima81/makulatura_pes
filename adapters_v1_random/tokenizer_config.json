{
  "additional_special_tokens": null,
  "backend": "tokenizers",
  "bos_token": "<|begin_of_text|>",
  "clean_up_tokenization_spaces": true,
  "eos_token": "<|eot_id|>",
  "is_local": true,
  "model_input_names": [
    "input_ids",
    "attention_mask"
  ],
  "model_max_length": 131072,
  "model_specific_special_tokens": {},
  "tokenizer_class": "TokenizersBackend"
}
